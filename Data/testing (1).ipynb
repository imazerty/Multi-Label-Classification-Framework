{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_raw_data()\n",
    "x = lower_case(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sentence length\n",
    "get_sentence_length(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking the sentences to 150 round\n",
    "x = chunk(x,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess >> lowercase, remove pun, remove html, keep only alphanumeric\n",
    "#optional _if you want to use stemmer on sentences then call stemming function after output of preprocessing\n",
    "#optional remove stop words \n",
    "\n",
    "#for better preprocessing replace some abbreviation with real words , see the text carefully then if there is for example\n",
    "#in medical data if there is AE in sentence replace with Adverse event something like that\n",
    "\n",
    "x = preproces(x)\n",
    "# x = steamit(x)\n",
    "# x = stopit_dude(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess finished \n",
    "# let's convert it indo dataframe\n",
    "df = labels_to_dataframe(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's decide how many labels we want to use\n",
    "#check label frequency\n",
    "\n",
    "#if you want to use manually then first call this function with all false values and check frequency then \n",
    "#give a frquency value to freq_value like i ddon't want any label which are not coming more than 5 times in whole corpus\n",
    "#if you want to use ratio method then just give 0.25 or ratio to keep ratio it will give 25% label of whole corpus\n",
    "# d_dataframe, frequcy_dict = keep_labels(df, keep_ratio = False, freq_Value = False)\n",
    "ration_data,freq_list = keep_labels(df, keep_ratio = 0.25, freq_Value = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj matrix \n",
    "adj_m = adj_matrix(ration_data)\n",
    "final = final_adj_matrix(adj_m,freq_list)\n",
    "#some tricks normalize the adj matrix or scale it into one range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's build the label encoding for magnet\n",
    "label_dict = get_label_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map it \n",
    "\n",
    "final_labels = []\n",
    "for labels in freq_list:\n",
    "    final_labels.append(label_dict[labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build label encoding now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = label_correlation_matrix(final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label corelation matrix, vocab, encoding\n",
    "# get vocab frequency then trim vocab basis on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocab\n",
    "\n",
    "#first pass the sentence to vocab function will all arguments false you'll get vocab frequency list then \n",
    "#decide how many words you want to keep in vocab\n",
    "#vocab fucntion will return four output > vocab_list with frequnecy, frequency list, word_to_int, int_to_word\n",
    "# vocab, freqs ,word_to_int,int_to_word = vocab_freq(x, keep_ratio = False, freq_Value = False, custom_value = False)\n",
    "#for example I am using all the words because it's under 30,000\n",
    "vocab, freqs ,word_to_int,int_to_word = vocab_freq(x, keep_ratio = False, freq_Value = False, custom_value = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the sentences\n",
    "#encoder return two output > encoded sentences, vocab dict\n",
    "\n",
    "encoded_sentences,vocab_dict = encoder(x,word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "# build char embedding function\n",
    "# build word2vec embedding\n",
    "# build elmo embedding\n",
    "# build bert embedding\n",
    "# all_embedding from tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab embedding\n",
    "# it will return two output vocab and words which are not in vocab and replaced with 'unk'\n",
    "#input should be tuple where tuple[0] = word and tuple[1] = encoding int from word _to_int\n",
    "#example vocab = [  ('hello',2),('world',1),('hello',0)   ]\n",
    "label_embedding, out_of_vocab_words = vocab_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
