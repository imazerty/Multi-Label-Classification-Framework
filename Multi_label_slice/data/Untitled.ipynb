{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quick_data(freq_label,how_much, pickle_ = True, baseline_classical = True, deep_learning = True):\n",
    "    \n",
    "    x,y = get_raw_data()\n",
    "    x = lower_case(x)\n",
    "    x = chunk(x,150)\n",
    "    x = preproces(x)\n",
    "    df = labels_to_dataframe(x,y)\n",
    "    ration_data, freq_list = keep_labels(df, keep_ratio = freq_label, freq_Value = False)\n",
    "    df = data_volume(ration_data,how_much)\n",
    "    ration_data_text = list(df['text'])\n",
    "    labels           = np.array(df.drop('text', 1))\n",
    "            \n",
    "    if baseline_classical:\n",
    "        X_train, X_val, y_train, y_val = split_data(ration_data_text,labels)\n",
    "        train_x, test_x = tf_idf(X_train,X_val)\n",
    "        \n",
    "        with open('baseline_classical_train_x.pkl','wb') as f:\n",
    "            pk.dump(train_x,f)\n",
    "        \n",
    "        with open('baseline_classical_y_train.pkl','wb') as f:\n",
    "            pk.dump(y_train,f)\n",
    "            \n",
    "        with open('baseline_classical_test_x.pkl','wb') as f:\n",
    "            pk.dump(test_x,f)\n",
    "            \n",
    "        with open('baseline_classical_y_val.pkl','wb') as f:\n",
    "            pk.dump(y_val,f)\n",
    "        \n",
    "        \n",
    "    if deep_learning:\n",
    "        sorted_long, freq_num,word_to_int,int_to_word = vocab_freq(ration_data_text)\n",
    "        all_sentences, vocab_dict = encoder(ration_data_text,word_to_int)\n",
    "        dgh = adj_matrix(ration_data)\n",
    "        final_ = final_adj_matrix(dgh,freq_list)\n",
    "        label_dict = get_label_dict()\n",
    "        #map it \n",
    "\n",
    "        final_labels = []\n",
    "        for labels_a in freq_list:\n",
    "            final_labels.append(label_dict[labels_a[0]])\n",
    "            \n",
    "        embeddin = label_correlation_matrix(final_labels)\n",
    "        \n",
    "        with open('deep_learning_all_sentences.pkl','wb') as f:\n",
    "            pk.dump(all_sentences,f)\n",
    "            \n",
    "        with open('deep_learning_all_labels.pkl','wb') as f:\n",
    "            pk.dump(labels,f)\n",
    "            \n",
    "        with open('deep_leaning_adj_matrix.pkl','wb') as f:\n",
    "            pk.dump(final_,f)\n",
    "            \n",
    "        with open('deep_leaning_label_embedding.pkl','wb') as f:\n",
    "            pk.dump(embeddin,f)\n",
    "            \n",
    "    return len(word_to_int), labels.shape\n",
    "\n",
    "# print(get_quick_data(0.25, 4000 , pickle_ = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ , freq_li = get_quick_data(0.25, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_data.pkl','wb') as f:\n",
    "    pk.dump(df_,f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/monk/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10788/10788 [00:02<00:00, 4336.20it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 104775.09it/s]\n",
      "100%|██████████| 4000/4000 [00:02<00:00, 1366.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 17266.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17523, (4000, 22))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for baseline classical machine leanring\n",
    "\n",
    "def baseline_classical(df_):\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_raw_data()\n",
    "x = lower_case(x)\n",
    "x = chunk(x,150)\n",
    "x = preproces(x)\n",
    "df = labels_to_dataframe(x,y)\n",
    "ration_data, freq_list = keep_labels(df, keep_ratio = 0.25, freq_Value = False)\n",
    "df = data_volume(ration_data,4000)\n",
    "ration_data_text = list(df['text'])\n",
    "labels           = np.array(df.drop('text', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
