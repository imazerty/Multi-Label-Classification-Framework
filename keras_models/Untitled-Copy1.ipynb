{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with elmo embedding\n",
    "# Import our dependencies\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model, load_model\n",
    "import keras.layers as layers\n",
    "from keras.engine import Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import os\n",
    "import re\n",
    "from keras import backend as K\n",
    "\n",
    "from graph_attention_layer import GraphAttention\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data\n",
    "# A, X, Y_train, Y_val, Y_test, idx_train, idx_val, idx_test = load_data('cora')\n",
    "\n",
    "X = np.random.uniform(0,9,[10,32])\n",
    "A = np.random.uniform(1,-1,[10,10])\n",
    "\n",
    "# # Parameters\n",
    "N = X.shape[0]                # Number of nodes in the graph\n",
    "F = X.shape[1]                # Original feature dimension\n",
    "n_classes = 10  # Number of classes\n",
    "F_ = 8                        # Output size of first GraphAttention layer\n",
    "n_attn_heads = 8              # Number of attention heads in first GAT layer\n",
    "dropout_rate = 0.6            # Dropout rate (between and inside GAT layers)\n",
    "l2_reg = 5e-4/2               # Factor for l2 regularization\n",
    "# learning_rate = 5e-3          # Learning rate for Adam\n",
    "# epochs = 10000                # Number of training epochs\n",
    "# es_patience = 100             # Patience fot early stopping\n",
    "\n",
    "# # Preprocessing operations\n",
    "# X = preprocess_features(X)\n",
    "A = A + np.eye(A.shape[0])  # Add self-loops\n",
    "\n",
    "# # Model definition (as per Section 3.3 of the paper)\n",
    "\n",
    "# constants = [1,2,3]\n",
    "# k_constants = K.variable(constants)\n",
    "# fixed_input = Input(tensor=k_constants)\n",
    "\n",
    "k_constants = K.variable(X)\n",
    "X_in = layers.Input(tensor=k_constants)\n",
    "ks_constants = K.variable(A)\n",
    "A_in = layers.Input(tensor=ks_constants)\n",
    "\n",
    "\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(50,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='softmax',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "\n",
    "new_model = Model([X_in, A_in], graph_attention_2, name='new_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'graph_attention_6/Softmax_1:0' shape=(10, 50) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_5/BiasAdd:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "#general vars\n",
    "length = 150\n",
    "\n",
    "\n",
    "dic_size = 100\n",
    "embed_size = 12\n",
    "\n",
    "input_text = Input(shape=(length,))\n",
    "embedding = Embedding(dic_size, embed_size)(input_text)\n",
    "\n",
    "embedding = LSTM(5)(embedding) \n",
    "embedding = Dense(50)(embedding)\n",
    "\n",
    "model_a = Model(input_text, embedding, name = 'model_a')\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (10, 32)             0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (10, 32)             0           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (10, 10)             0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 150, 12)      1200        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_9 (GraphAttenti (10, 64)             2240        dropout_81[0][0]                 \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 5)            360         embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (10, 64)             0           graph_attention_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50)           300         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_10 (GraphAttent (10, 50)             3350        dropout_98[0][0]                 \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 10)           0           dense_5[0][0]                    \n",
      "                                                                 graph_attention_10[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 7,450\n",
      "Trainable params: 7,450\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mult = Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([embedding, graph_attention_2])\n",
    "\n",
    "# model = Model([input_text, dummyInput], mult, name='full_model')\n",
    "\n",
    "final_model = Model(inputs=[model_a.input,new_model.input[0],new_model.input[1]], outputs=mult)\n",
    "\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build model\n",
    "\n",
    "num_classes = len(set(final['labels']))\n",
    "\n",
    "def build_model(): \n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    dense = layers.Dense(256, activation='relu')(embedding)\n",
    "    pred = layers.Dense(num_classes, activation='softmax')(dense)\n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
